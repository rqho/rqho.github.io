1:"$Sreact.fragment"
2:I[1968,["968","static/chunks/968-0853dab5531ea4ad.js","57","static/chunks/57-4430c49958c95c44.js","419","static/chunks/app/projects/%5Bslug%5D/page-267156bd979ce146.js"],""]
3:I[6221,[],""]
4:I[5657,[],""]
5:I[1432,["968","static/chunks/968-0853dab5531ea4ad.js","177","static/chunks/app/layout-af0718d74a60bc39.js"],"Analytics"]
6:I[6117,["968","static/chunks/968-0853dab5531ea4ad.js","177","static/chunks/app/layout-af0718d74a60bc39.js"],"SpeedInsights"]
8:I[8002,[],"OutletBoundary"]
a:I[8002,[],"MetadataBoundary"]
c:I[8002,[],"ViewportBoundary"]
e:I[4404,[],""]
:HL["/_next/static/media/806de4d605d3ad01-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/fc727f226c737876-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/a43e181e027238b3.css","style"]
0:{"P":null,"b":"0dt51y9k5uBUiGq1Rybkx","p":"","c":["","projects","neural-network-accelerator"],"i":false,"f":[[["",{"children":["projects",{"children":[["slug","neural-network-accelerator","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/a43e181e027238b3.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"text-white bg-[rgb(16,16,16)] __variable_e8b655","children":["$","body",null,{"className":"antialiased max-w-2xl mx-4 mt-8 lg:mx-auto font-mono","children":[["$","link",null,{"rel":"stylesheet","href":"https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css","crossOrigin":"anonymous"}],["$","main",null,{"className":"flex-auto min-w-0 mt-6 flex flex-col px-2 md:px-0","children":[["$","aside",null,{"className":"-ml-[8px] mb-16 tracking-tight","children":["$","div",null,{"className":"lg:sticky lg:top-20","children":["$","nav",null,{"className":"flex flex-row items-start relative px-0 pb-0 fade md:overflow-auto scroll-pr-6 md:relative","id":"nav","children":["$","div",null,{"className":"flex flex-row space-x-0 pr-10","children":[["$","$L2","/",{"href":"/","className":"transition-all hover:text-neutral-200 flex align-middle relative py-1 px-2 m-1","children":"Home"}],["$","$L2","/blog",{"href":"/blog","className":"transition-all hover:text-neutral-200 flex align-middle relative py-1 px-2 m-1","children":"Blog"}],["$","$L2","/projects",{"href":"/projects","className":"transition-all hover:text-neutral-200 flex align-middle relative py-1 px-2 m-1","children":"Projects"}]]}]}]}]}],["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],["$","section",null,{"children":[["$","h1",null,{"className":"mb-8 text-2xl font-semibold tracking-tighter","children":"404 - Page Not Found"}],["$","p",null,{"className":"mb-4","children":"The page you are looking for does not exist."}]]}]],"forbidden":"$undefined","unauthorized":"$undefined"}],["$","footer",null,{"className":"mb-16","children":[["$","ul",null,{"className":"font-sm mt-8 flex flex-col space-x-0 space-y-2 text-neutral-300 md:flex-row md:space-x-4 md:space-y-0","children":[["$","li",null,{"children":["$","a",null,{"className":"flex items-center transition-all hover:text-neutral-100","rel":"noopener noreferrer","target":"_blank","href":"/rss","children":[["$","svg",null,{"width":"12","height":"12","viewBox":"0 0 12 12","fill":"none","xmlns":"http://www.w3.org/2000/svg","children":["$","path",null,{"d":"M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z","fill":"currentColor"}]}],["$","p",null,{"className":"ml-2 h-7","children":"rss"}]]}]}],["$","li",null,{"children":["$","a",null,{"className":"flex items-center transition-all hover:text-neutral-100","rel":"noopener noreferrer","target":"_blank","href":"https://github.com/rqho","children":[["$","svg",null,{"width":"12","height":"12","viewBox":"0 0 12 12","fill":"none","xmlns":"http://www.w3.org/2000/svg","children":["$","path",null,{"d":"M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z","fill":"currentColor"}]}],["$","p",null,{"className":"ml-2 h-7","children":"GitHub"}]]}]}],["$","li",null,{"children":["$","a",null,{"className":"flex items-center transition-all hover:text-neutral-100","rel":"noopener noreferrer","target":"_blank","href":"https://linkedin.com/in/rqho","children":[["$","svg",null,{"width":"12","height":"12","viewBox":"0 0 12 12","fill":"none","xmlns":"http://www.w3.org/2000/svg","children":["$","path",null,{"d":"M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z","fill":"currentColor"}]}],["$","p",null,{"className":"ml-2 h-7","children":"LinkedIn"}]]}]}]]}],["$","p",null,{"className":"mt-8 text-neutral-300","children":["© ",2025," Richard Ho"]}]]}],["$","$L5",null,{}],["$","$L6",null,{}]]}]]}]}]]}],{"children":["projects",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","projects","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","neural-network-accelerator","d"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","projects","children","$0:f:0:1:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L7",null,["$","$L8",null,{"children":"$L9"}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","T74MGri6Qb3gnULNw9sug",{"children":[["$","$La",null,{"children":"$Lb"}],["$","$Lc",null,{"children":"$Ld"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$e","$undefined"],"s":false,"S":true}
7:["$","section",null,{"children":[["$","script",null,{"type":"application/ld+json","suppressHydrationWarning":true,"dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BlogPosting\",\"headline\":\"FPGA Neural Network Accelerator\",\"datePublished\":\"2024-08-10\",\"dateModified\":\"2024-08-10\",\"description\":\"Custom FPGA-based neural network accelerator optimized for edge computing applications with focus on power efficiency and real-time inference.\",\"image\":\"https://images.unsplash.com/photo-1597862624292-45748390b00e?q=80&w=2940&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\",\"url\":\"https://rqho.github.io/projects/neural-network-accelerator\",\"author\":{\"@type\":\"Person\",\"name\":\"Richard Ho\"}}"}}],["$","div",null,{"className":"mb-8","children":[["$","h1",null,{"className":"title font-semibold text-2xl tracking-tighter mb-4","children":"FPGA Neural Network Accelerator"}],["$","div",null,{"className":"flex flex-col sm:flex-row sm:items-center gap-4 mb-6","children":[["$","div",null,{"className":"flex items-center gap-4 text-sm text-neutral-400","children":[["$","time",null,{"dateTime":"2024-08-10","children":"August 10, 2024"}],["$","span",null,{"className":"px-2 py-1 text-xs rounded-full bg-green-500/20 text-green-400","children":["✓ Completed",false,false]}]]}],"$undefined"]}],["$","div",null,{"className":"mb-6","children":[["$","h3",null,{"className":"text-sm font-medium text-neutral-300 mb-2","children":"Technologies"}],["$","div",null,{"className":"flex flex-wrap gap-2","children":[["$","span","Verilog",{"className":"px-3 py-1 text-sm bg-neutral-800 text-neutral-300 rounded-md","children":"Verilog"}],["$","span","FPGA",{"className":"px-3 py-1 text-sm bg-neutral-800 text-neutral-300 rounded-md","children":"FPGA"}],["$","span","Python",{"className":"px-3 py-1 text-sm bg-neutral-800 text-neutral-300 rounded-md","children":"Python"}],["$","span","TensorFlow",{"className":"px-3 py-1 text-sm bg-neutral-800 text-neutral-300 rounded-md","children":"TensorFlow"}],["$","span","Computer Architecture",{"className":"px-3 py-1 text-sm bg-neutral-800 text-neutral-300 rounded-md","children":"Computer Architecture"}],["$","span","Digital Design",{"className":"px-3 py-1 text-sm bg-neutral-800 text-neutral-300 rounded-md","children":"Digital Design"}]]}]]}],["$","div",null,{"className":"flex gap-4 mb-8","children":[["$","a",null,{"href":"https://github.com/rqho/fpga-nn-accelerator","target":"_blank","rel":"noopener noreferrer","className":"inline-flex items-center gap-2 px-4 py-2 bg-neutral-800 hover:bg-neutral-700 text-neutral-200 rounded-md transition-colors text-sm","children":"GitHub ↗"}],"$undefined"]}],["$","div",null,{"className":"mb-8","children":["$","img",null,{"src":"https://images.unsplash.com/photo-1597862624292-45748390b00e?q=80&w=2940&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D","alt":"FPGA Neural Network Accelerator","className":"w-full h-64 object-cover rounded-lg border border-neutral-800"}]}]]}],["$","article",null,{"className":"prose prose-invert max-w-none","children":"$Lf"}]]}]
f:[["$","h1",null,{"id":"fpga-neural-network-accelerator","children":[[["$","a","link-fpga-neural-network-accelerator",{"href":"#fpga-neural-network-accelerator","className":"anchor"}]],"FPGA Neural Network Accelerator"]}],"\n",["$","p",null,{"className":"text-sm leading-relaxed mb-4","children":"A custom hardware accelerator designed for efficient neural network inference on edge devices. This project explores the intersection of computer architecture and machine learning, focusing on optimizing both performance and power consumption."}],"\n",["$","h2",null,{"id":"architecture-overview","children":[[["$","a","link-architecture-overview",{"href":"#architecture-overview","className":"anchor"}]],"Architecture Overview"]}],"\n",["$","p",null,{"className":"text-sm leading-relaxed mb-4","children":"The accelerator implements a systolic array architecture optimized for matrix multiplication operations that are fundamental to neural network computations."}],"\n",["$","h3",null,{"id":"design-features","children":[[["$","a","link-design-features",{"href":"#design-features","className":"anchor"}]],"Design Features"]}],"\n",["$","ul",null,{"className":"list-disc list-outside space-y-2 pl-6 text-sm","children":["\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Parallel Processing Units"}],": 16x16 systolic array for matrix operations"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Custom Memory Hierarchy"}],": Optimized for neural network data access patterns"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Quantization Support"}],": 8-bit and 16-bit fixed-point arithmetic"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Dynamic Reconfiguration"}],": Adaptable to different network architectures"]}],"\n"]}],"\n",["$","h2",null,{"id":"performance-optimizations","children":[[["$","a","link-performance-optimizations",{"href":"#performance-optimizations","className":"anchor"}]],"Performance Optimizations"]}],"\n",["$","h3",null,{"id":"memory-access-optimization","children":[[["$","a","link-memory-access-optimization",{"href":"#memory-access-optimization","className":"anchor"}]],"Memory Access Optimization"]}],"\n",["$","ul",null,{"className":"list-disc list-outside space-y-2 pl-6 text-sm","children":["\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Data Reuse"}],": Maximizes utilization of on-chip memory"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Prefetching"}],": Anticipates memory access patterns to reduce latency"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Compression"}],": On-the-fly weight compression to reduce bandwidth requirements"]}],"\n"]}],"\n",["$","h3",null,{"id":"computational-efficiency","children":[[["$","a","link-computational-efficiency",{"href":"#computational-efficiency","className":"anchor"}]],"Computational Efficiency"]}],"\n",["$","ul",null,{"className":"list-disc list-outside space-y-2 pl-6 text-sm","children":["\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Pipeline Design"}],": Deep pipeline for maximum throughput"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Parallel Execution"}],": Multiple operations per clock cycle"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Energy Optimization"}],": Clock gating and power islands for unused components"]}],"\n"]}],"\n",["$","h2",null,{"id":"benchmarks-and-results","children":[[["$","a","link-benchmarks-and-results",{"href":"#benchmarks-and-results","className":"anchor"}]],"Benchmarks and Results"]}],"\n",["$","p",null,{"className":"text-sm leading-relaxed mb-4","children":"Tested on standard neural network benchmarks:"}],"\n",["$","h3",null,{"id":"cnn-performance-resnet-18","children":[[["$","a","link-cnn-performance-resnet-18",{"href":"#cnn-performance-resnet-18","className":"anchor"}]],"CNN Performance (ResNet-18)"]}],"\n",["$","ul",null,{"className":"list-disc list-outside space-y-2 pl-6 text-sm","children":["\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Throughput"}],": 1,200 images/second at 100 MHz"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Power Consumption"}],": 2.3W (10x more efficient than GPU)"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Accuracy"}],": 99.2% of full-precision baseline"]}],"\n"]}],"\n",["$","h3",null,{"id":"edge-deployment","children":[[["$","a","link-edge-deployment",{"href":"#edge-deployment","className":"anchor"}]],"Edge Deployment"]}],"\n",["$","ul",null,{"className":"list-disc list-outside space-y-2 pl-6 text-sm","children":["\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Latency"}],": 0.83ms per inference"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Energy"}],": 1.9mJ per inference"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Memory"}],": 512KB on-chip storage"]}],"\n"]}],"\n",["$","h2",null,{"id":"applications","children":[[["$","a","link-applications",{"href":"#applications","className":"anchor"}]],"Applications"]}],"\n",["$","p",null,{"className":"text-sm leading-relaxed mb-4","children":"The accelerator has been successfully deployed in:"}],"\n",["$","ul",null,{"className":"list-disc list-outside space-y-2 pl-6 text-sm","children":["\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Autonomous Vehicles"}],": Real-time object detection"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"IoT Devices"}],": Edge AI processing with battery constraints"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Robotics"}],": Low-latency perception systems"]}],"\n"]}],"\n",["$","h2",null,{"id":"technical-innovation","children":[[["$","a","link-technical-innovation",{"href":"#technical-innovation","className":"anchor"}]],"Technical Innovation"]}],"\n",["$","h3",null,{"id":"novel-contributions","children":[[["$","a","link-novel-contributions",{"href":"#novel-contributions","className":"anchor"}]],"Novel Contributions"]}],"\n",["$","ol",null,{"className":"list-decimal list-outside space-y-2 pl-6 ml-2 text-sm","children":["\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Adaptive Quantization"}],": Dynamic bit-width adjustment based on layer sensitivity"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Hierarchical Memory Design"}],": Three-level memory hierarchy optimized for NN workloads"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Runtime Reconfiguration"}],": Hardware adaptation to different network topologies"]}],"\n"]}],"\n",["$","h3",null,{"id":"synthesis-results","children":[[["$","a","link-synthesis-results",{"href":"#synthesis-results","className":"anchor"}]],"Synthesis Results"]}],"\n",["$","ul",null,{"className":"list-disc list-outside space-y-2 pl-6 text-sm","children":["\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Target FPGA"}],": Xilinx Zynq UltraScale+"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Logic Utilization"}],": 78% LUTs, 65% DSP blocks"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Maximum Frequency"}],": 150 MHz"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"On-chip Memory"}],": 512KB BRAM"]}],"\n"]}],"\n",["$","h2",null,{"id":"future-enhancements","children":[[["$","a","link-future-enhancements",{"href":"#future-enhancements","className":"anchor"}]],"Future Enhancements"]}],"\n",["$","p",null,{"className":"text-sm leading-relaxed mb-4","children":"Currently investigating:"}],"\n",["$","ul",null,{"className":"list-disc list-outside space-y-2 pl-6 text-sm","children":["\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Transformer Architecture Support"}],": Optimizations for attention mechanisms"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Sparse Network Acceleration"}],": Hardware support for pruned networks"]}],"\n",["$","li",null,{"className":"leading-relaxed text-sm","children":[["$","strong",null,{"children":"Multi-precision Arithmetic"}],": Dynamic precision scaling during inference"]}],"\n"]}],"\n",["$","h2",null,{"id":"publications","children":[[["$","a","link-publications",{"href":"#publications","className":"anchor"}]],"Publications"]}],"\n",["$","p",null,{"className":"text-sm leading-relaxed mb-4","children":"This work contributed to a paper submitted to the International Symposium on Computer Architecture (ISCA) focusing on energy-efficient neural network acceleration in edge computing environments."}]]
d:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
b:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"FPGA Neural Network Accelerator | Next.js Portfolio Starter"}],["$","meta","2",{"name":"description","content":"Custom FPGA-based neural network accelerator optimized for edge computing applications with focus on power efficiency and real-time inference."}],["$","meta","3",{"name":"robots","content":"index, follow"}],["$","meta","4",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","meta","5",{"name":"katex-css","content":"https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css"}],["$","meta","6",{"property":"og:title","content":"FPGA Neural Network Accelerator"}],["$","meta","7",{"property":"og:description","content":"Custom FPGA-based neural network accelerator optimized for edge computing applications with focus on power efficiency and real-time inference."}],["$","meta","8",{"property":"og:url","content":"https://rqho.github.io/projects/neural-network-accelerator"}],["$","meta","9",{"property":"og:image","content":"https://images.unsplash.com/photo-1597862624292-45748390b00e?q=80&w=2940&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"}],["$","meta","10",{"property":"og:type","content":"article"}],["$","meta","11",{"property":"article:published_time","content":"2024-08-10"}],["$","meta","12",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","13",{"name":"twitter:title","content":"FPGA Neural Network Accelerator"}],["$","meta","14",{"name":"twitter:description","content":"Custom FPGA-based neural network accelerator optimized for edge computing applications with focus on power efficiency and real-time inference."}],["$","meta","15",{"name":"twitter:image","content":"https://images.unsplash.com/photo-1597862624292-45748390b00e?q=80&w=2940&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"}]]
9:null
